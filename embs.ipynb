{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e4bfb",
   "metadata": {
    "cellId": "eo8juvynrpu1g90njw3coe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba757b2",
   "metadata": {
    "cellId": "egufw04b7reaxk00lg7nh"
   },
   "outputs": [],
   "source": [
    "%cd voxceleb_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9a747",
   "metadata": {
    "cellId": "gcsjo7hbtzsz7lu1x5iqah"
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87482ef",
   "metadata": {
    "cellId": "74st758ph7fcyzlwwx78ke"
   },
   "outputs": [],
   "source": [
    "!python3 ./dataprep.py --save_path ??? --download --user ??? --password ???  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261433d9",
   "metadata": {
    "cellId": "78rp2p891x4xevh8v7z7u"
   },
   "outputs": [],
   "source": [
    "!python3 ./dataprep.py --save_path /home/jupyter/work/resources/voxceleb_trainer/data --extract\n",
    "!python3 ./dataprep.py --save_path /home/jupyter/work/resources/voxceleb_trainer/data --convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ce51f",
   "metadata": {
    "cellId": "6onzlhczjt313q71jei80n"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "!python3 ./trainSpeakerNet.py --eval --model ResNetSE34V2 --log_input True --encoder_type ASP --n_mels 64 --trainfunc softmaxproto --save_path /home/jupyter/work/resources/voxceleb_trainer/exps/test --eval_frames 400  --initial_model baseline_v2_ap.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c6ba37",
   "metadata": {
    "cellId": "pcpnuo051wnzcp2ksl1m9q"
   },
   "source": [
    "## Генерируем семьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97dbd6e",
   "metadata": {
    "cellId": "b7lndp82y3swq8crrm932b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "embs = torch.load('embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a675b8",
   "metadata": {
    "cellId": "jxtapjzslkct8zln4jgly8"
   },
   "outputs": [],
   "source": [
    "#делаем словарь вида {'id' : усредненный embedding диктора}\n",
    "\n",
    "utterance_list = open('./data/train_list.txt', 'r')\n",
    "\n",
    "mean_embs = dict()\n",
    "current_id = ''\n",
    "emb_sum, count = 0, 0\n",
    "\n",
    "for line in utterance_list:\n",
    "    id, path = line.split()\n",
    "    \n",
    "    if id == current_id:\n",
    "        emb_sum += embs[path]\n",
    "        count += 1\n",
    "    else:\n",
    "        if current_id:\n",
    "            mean_embs[current_id] = 1 / count * emb_sum\n",
    "\n",
    "        emb_sum = embs[path]\n",
    "        count = 1\n",
    "        current_id = id\n",
    "        \n",
    "    mean_embs[current_id] = 1 / count * emb_sum\n",
    "        \n",
    "utterance_list.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94526c5",
   "metadata": {
    "cellId": "vj7rqpggnzfu3lgyetf38"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#score каждого диктора с каждым\n",
    "import numpy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "scores = dict()\n",
    "ids = list(mean_embs.keys())\n",
    "\n",
    "for i in range(len(ids)):\n",
    "    for j in range(i + 1, len(ids)):\n",
    "        \n",
    "        ref_feat = mean_embs[ids[i]].cuda()\n",
    "        com_feat = mean_embs[ids[j]].cuda()\n",
    "\n",
    "        dist = F.cosine_similarity(ref_feat.unsqueeze(-1), com_feat.unsqueeze(-1).transpose(0,2)).detach().cpu().numpy();\n",
    "        score = numpy.mean(dist);\n",
    "        scores[(ids[i], ids[j])] = score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c09db",
   "metadata": {
    "cellId": "6j1zizqabvnux67he3z9e"
   },
   "outputs": [],
   "source": [
    "similarity_scores = list(scores.values())\n",
    "min_score = min(similarity_scores)\n",
    "max_score = max(similarity_scores)\n",
    "print(min_score, max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a6e6a",
   "metadata": {
    "cellId": "846jnjw3ced28txyss3mft"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#делаем матрицу scores\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "ids = list(mean_embs.keys())\n",
    "\n",
    "n = len(ids)\n",
    "scores = np.zeros((n, n))\n",
    "id2ind = {ids[i] : i for i in range(n)}\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        \n",
    "        ref_feat = mean_embs[ids[i]].cuda()\n",
    "        com_feat = mean_embs[ids[j]].cuda()\n",
    "\n",
    "        dist = F.cosine_similarity(ref_feat.unsqueeze(-1), com_feat.unsqueeze(-1).transpose(0,2)).detach().cpu().numpy();\n",
    "        score = np.mean(dist);\n",
    "        scores[id2ind[ids[i]], id2ind[ids[j]]] = score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a29835",
   "metadata": {
    "cellId": "6ynlmqshbrrzdgwrxwqmqf"
   },
   "outputs": [],
   "source": [
    "scores = scores + scores.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8439445",
   "metadata": {
    "cellId": "lomf9kc9paig6oks3mp5bf"
   },
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "ind2id = {i : ids[i] for i in range(n)}\n",
    "\n",
    "components_mat = scores > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc1869",
   "metadata": {
    "cellId": "umwonuc5x1l5dypmpqmte"
   },
   "outputs": [],
   "source": [
    "def generate_family(member_id, member_count, components_mat, id2ind, ind2id, relatives_set=set(ids)):\n",
    "    \n",
    "    family = {member_id}\n",
    "    \n",
    "    while len(family) < len(relatives_set):\n",
    "        related = np.nonzero(components_mat[id2ind[member_id], :])[0]\n",
    "        related_ids = [ind2id[i] for i in related]\n",
    "\n",
    "        relatives_set &= set(related_ids)\n",
    "        \n",
    "        if len(family) == member_count:\n",
    "            return family\n",
    "        if not relatives_set:\n",
    "            return family\n",
    "        \n",
    "        add_member_id = np.random.choice(list(relatives_set))\n",
    "        \n",
    "        #print('current member ', member_id, ' related to ', relatives_set, ' chosen connection to ', add_member_id)\n",
    "        \n",
    "        family.add(add_member_id)\n",
    "        member_id = add_member_id\n",
    "        \n",
    "    return family\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48ed44",
   "metadata": {
    "cellId": "zugg6dgsdwi4x71i8er8ta"
   },
   "outputs": [],
   "source": [
    "families = []\n",
    "\n",
    "for member_count in range(2, 8):\n",
    "    fixed_size_families = []\n",
    "    for id_ in ids:\n",
    "        family = generate_family(id_, member_count, components_mat.astype(int), id2ind, ind2id, relatives_set=set(ids))\n",
    "        if len(family) == member_count:\n",
    "            fixed_size_families.append(family)\n",
    "    families.append(fixed_size_families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e0bb59",
   "metadata": {
    "cellId": "uilo79233hfmlpn1z5utk"
   },
   "outputs": [],
   "source": [
    "#generate_family('id10104', components_mat.astype(int), id2ind, ind2id, relatives_set=set(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b44f8",
   "metadata": {
    "cellId": "mtblgjqogrsqorecobyuj"
   },
   "outputs": [],
   "source": [
    "for i, fixed_families in enumerate(families):\n",
    "    print('nSpeakers', i + 2, len(np.unique(fixed_families)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf23cc81",
   "metadata": {
    "cellId": "p4h5vx3ts7sf8uch7cgpl5"
   },
   "source": [
    "## Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b7631",
   "metadata": {
    "cellId": "095uusrw6bslc03lkp1zx05"
   },
   "outputs": [],
   "source": [
    "def tuneThresholdfromScore(scores, labels, target_fa, target_fr = None):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    tunedThreshold = [];\n",
    "    if target_fr:\n",
    "        for tfr in target_fr:\n",
    "            idx = numpy.nanargmin(numpy.absolute((tfr - fnr)))\n",
    "            tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]]);\n",
    "    \n",
    "    for tfa in target_fa:\n",
    "        idx = numpy.nanargmin(numpy.absolute((tfa - fpr))) # numpy.where(fpr<=tfa)[0][-1]\n",
    "        tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]]);\n",
    "    \n",
    "    idxE = numpy.nanargmin(numpy.absolute((fnr - fpr)))\n",
    "    eer  = max(fpr[idxE],fnr[idxE])*100\n",
    "    \n",
    "    return (tunedThreshold, eer, fpr, fnr);\n",
    "\n",
    "# Creates a list of false-negative rates, a list of false-positive rates\n",
    "# and a list of decision thresholds that give those error-rates.\n",
    "\n",
    "\n",
    "def ComputeErrorRates(scores, labels):\n",
    "\n",
    "      # Sort the scores from smallest to largest, and also get the corresponding\n",
    "      # indexes of the sorted scores.  We will treat the sorted scores as the\n",
    "      # thresholds at which the the error-rates are evaluated.\n",
    "      sorted_indexes, thresholds = zip(*sorted(\n",
    "          [(index, threshold) for index, threshold in enumerate(scores)],\n",
    "          key=itemgetter(1)))\n",
    "      sorted_labels = []\n",
    "      labels = [labels[i] for i in sorted_indexes]\n",
    "      fnrs = []\n",
    "      fprs = []\n",
    "\n",
    "      # At the end of this loop, fnrs[i] is the number of errors made by\n",
    "      # incorrectly rejecting scores less than thresholds[i]. And, fprs[i]\n",
    "      # is the total number of times that we have correctly accepted scores\n",
    "      # greater than thresholds[i].\n",
    "      for i in range(0, len(labels)):\n",
    "          if i == 0:\n",
    "              fnrs.append(labels[i])\n",
    "              fprs.append(1 - labels[i])\n",
    "          else:\n",
    "              fnrs.append(fnrs[i-1] + labels[i])\n",
    "              fprs.append(fprs[i-1] + 1 - labels[i])\n",
    "      fnrs_norm = sum(labels)\n",
    "      fprs_norm = len(labels) - fnrs_norm\n",
    "\n",
    "      # Now divide by the total number of false negative errors to\n",
    "      # obtain the false positive rates across all thresholds\n",
    "      fnrs = [x / float(fnrs_norm) for x in fnrs]\n",
    "\n",
    "      # Divide by the total number of corret positives to get the\n",
    "      # true positive rate.  Subtract these quantities from 1 to\n",
    "      # get the false positive rates.\n",
    "      fprs = [1 - x / float(fprs_norm) for x in fprs]\n",
    "      return fnrs, fprs, thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65ac1d",
   "metadata": {
    "cellId": "flhkwr1c3qgq1xl0tkzyt"
   },
   "outputs": [],
   "source": [
    "def false_acceptance_rate(labels, labels_true):\n",
    "    labels, labels_true = np.array(labels), np.array(labels_true)\n",
    "    return ((labels != 'g') & (labels_true == 'g')).sum() / len(labels) \n",
    "\n",
    "\n",
    "def false_rejection_rate(labels, labels_true):\n",
    "    labels, labels_true = np.array(labels), np.array(labels_true)\n",
    "    return ((labels == 'g') & (labels_true != 'g')).sum() / len(labels) \n",
    "\n",
    "\n",
    "def wrong_member_rate(labels, labels_true):\n",
    "    labels, labels_true = np.array(labels), np.array(labels_true)\n",
    "    return ((labels != 'g') & (labels_true != 'g') & (labels != labels_true)).sum() / (labels_true != 'g').sum() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b6bea",
   "metadata": {
    "cellId": "xtfvrwwuswf22waur1vo04"
   },
   "outputs": [],
   "source": [
    "#сделаем словарь вида {id : [list of path to records]}\n",
    "\n",
    "f = open('./data/train_list.txt', 'r')\n",
    "id2records = {ids[i] : [] for i in range(len(ids))}\n",
    "\n",
    "for line in f:\n",
    "    id_, path = line.split()\n",
    "    id2records[id_].append(path)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c9f28",
   "metadata": {
    "cellId": "429ilcxv68tzb27n5vd3ak"
   },
   "outputs": [],
   "source": [
    "#сделаем пробную генерацию выборки и пропустим ее через tuneThresholdfromScore\n",
    "#необходимо сделать так, чтобы эмбеддинги из enrolls не пересекались с эмбеддингами из dataset\n",
    "\n",
    "#member_id - person id \n",
    "#count - len(list of utterances from person_id)\n",
    "\n",
    "def sample_utterance(member_id, count, except_paths=[]):\n",
    "    path_records = list(np.random.choice(list(set(id2records[member_id]) - set(except_paths)), count))\n",
    "    sample = [embs[path] for path in path_records]\n",
    "    return sample, path_records\n",
    "\n",
    "def generate_enrolls(family_ids, level=4):\n",
    "    enrolls = dict.fromkeys(family_ids)\n",
    "    for member_id in family_ids:\n",
    "        utterances, path_records = sample_utterance(member_id, level)\n",
    "        enrolls[member_id] = sum(utterances) / level\n",
    "    return enrolls, path_records\n",
    "    \n",
    "from sklearn.utils import shuffle  \n",
    "def generate_dataset(family_ids, enrolled_paths, ut_per_member=10, ut_per_guest=4, guests_count=4):\n",
    "    utterances, labels, used_records = [], [], []\n",
    "    for member_id in family_ids:\n",
    "        sample, paths = sample_utterance(member_id, ut_per_member, enrolled_paths)\n",
    "        \n",
    "        utterances += sample\n",
    "        used_records += paths\n",
    "        labels += [member_id] * ut_per_member\n",
    "        \n",
    "    guest_ids = np.random.choice(list(set(ids) - set(family_ids)), guests_count)\n",
    "    for guest_id in guest_ids:\n",
    "        sample, paths = sample_utterance(guest_id, ut_per_guest, enrolled_paths)\n",
    "        utterances += sample\n",
    "        used_records += paths\n",
    "        labels += ['g'] * ut_per_guest\n",
    "    \n",
    "    utterances, labels = shuffle(utterances, labels)\n",
    "    return utterances, labels, used_records\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d63d908",
   "metadata": {
    "cellId": "zsxuztyqokigviwvs4pltk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#берем по 100 примеров каждой семьи\n",
    "import torch.nn.functional as F\n",
    "\n",
    "mean_eers = []\n",
    "\n",
    "for i in range(len(families)):\n",
    "    eers = []\n",
    "    for j in range(1000):\n",
    "        fam = list(families[i][j])\n",
    "        enrolls, enrolls_path = generate_enrolls(fam)\n",
    "        utts, labels_true, _ = generate_dataset(fam, enrolls_path)\n",
    "\n",
    "        scores = []\n",
    "        labels = []\n",
    "\n",
    "        for utterance in utts:\n",
    "            scores_with_enrolled = []\n",
    "            for enrolled_id, enrolled_ut in enrolls.items():\n",
    "                ref_feat = utterance.cuda()\n",
    "                com_feat = enrolled_ut.cuda()\n",
    "                dist = F.cosine_similarity(ref_feat.unsqueeze(-1), com_feat.unsqueeze(-1).transpose(0,2)).detach().cpu().numpy();\n",
    "                score = numpy.mean(dist);\n",
    "                scores_with_enrolled.append(score)\n",
    "\n",
    "            scores.append(max(scores_with_enrolled))\n",
    "            labels.append(list(enrolls.keys())[np.argmax(scores_with_enrolled)])\n",
    "\n",
    "\n",
    "        far, frr, wmr = [], [], []\n",
    "\n",
    "        thresholds = sorted(scores)\n",
    "        labels, scores = np.array(labels), np.array(scores)\n",
    "\n",
    "        for t in thresholds:\n",
    "            labels[np.where(scores <= t)[0]] = 'g'\n",
    "            far.append(false_acceptance_rate(labels, labels_true))\n",
    "            frr.append(false_rejection_rate(labels, labels_true))\n",
    "            wmr.append(wrong_member_rate(labels, labels_true))\n",
    "\n",
    "        fnir, far = np.array(frr) + np.array(wmr), np.array(far)\n",
    "        idxE = np.nanargmin(np.absolute((far - fnir)))\n",
    "        eer  = max(far[idxE],fnir[idxE])*100\n",
    "        eers.append(eer)\n",
    "    mean_eers.append(np.mean(eers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d8b0b",
   "metadata": {
    "cellId": "sjb1u93o5m5mqmr10q0a"
   },
   "outputs": [],
   "source": [
    "#по 100 семей\n",
    "for i in range(len(mean_eers)):\n",
    "    print('nSpeakers =', i + 2, 'EER =', mean_eers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e2c58",
   "metadata": {
    "cellId": "ao3vtwsmu0siohbdrs7etm"
   },
   "outputs": [],
   "source": [
    "#по 1000 семей\n",
    "for i in range(len(mean_eers)):\n",
    "    print('nSpeakers =', i + 2, 'EER =', mean_eers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c04f59",
   "metadata": {
    "cellId": "5wbmwyxmd6myymt8t0kzrp"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_score(emb_1, emb_2):\n",
    "    ref_feat = emb_1.cuda()\n",
    "    com_feat = emb_2.cuda()\n",
    "    dist = F.cosine_similarity(ref_feat.unsqueeze(-1), com_feat.unsqueeze(-1).transpose(0,2)).detach().cpu().numpy();\n",
    "    score = numpy.mean(dist);\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66affc75",
   "metadata": {
    "cellId": "ifkmtmbg67bv7cb6b55sz9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "simulations = 100\n",
    "grid_search = []\n",
    "\n",
    "for a in np.linspace(0.75, 0.95, 3):\n",
    "    for threshold in np.linspace(0.4, 0.8, 5):\n",
    "\n",
    "        mean_eers = []\n",
    "        for i in range(len(families)):\n",
    "            eers = []\n",
    "            for j in range(simulations):\n",
    "                fam = list(families[i][j])\n",
    "\n",
    "                enrolls, enrolls_path = generate_enrolls(fam)\n",
    "                train_utts, _, used_in_train = generate_dataset(fam, enrolls_path)\n",
    "                utts, labels_true, _ = generate_dataset(fam, enrolls_path + used_in_train)\n",
    "\n",
    "                #training enrolls\n",
    "                for utterance in train_utts:\n",
    "                    for enrolled_id, enrolled_ut in enrolls.items():\n",
    "\n",
    "                        if cosine_score(utterance, enrolled_ut) > threshold:\n",
    "                            enrolls[enrolled_id] = F.normalize(a * enrolled_ut + (1 - a) * utterance, p=2, dim=1)\n",
    "\n",
    "                #metrics on test dataset\n",
    "\n",
    "                scores, labels = [], []\n",
    "\n",
    "                for utterance in utts:\n",
    "                    scores_with_enrolled = []\n",
    "\n",
    "                    for enrolled_id, enrolled_ut in enrolls.items():\n",
    "                        scores_with_enrolled.append(cosine_score(utterance, enrolled_ut))\n",
    "\n",
    "                    scores.append(max(scores_with_enrolled))\n",
    "                    labels.append(list(enrolls.keys())[np.argmax(scores_with_enrolled)])\n",
    "\n",
    "\n",
    "                far, frr, wmr = [], [], []\n",
    "\n",
    "                thresholds = sorted(scores)\n",
    "                labels, scores = np.array(labels), np.array(scores)\n",
    "\n",
    "                for t in thresholds:\n",
    "                    labels[np.where(scores <= t)[0]] = 'g'\n",
    "                    far.append(false_acceptance_rate(labels, labels_true))\n",
    "                    frr.append(false_rejection_rate(labels, labels_true))\n",
    "                    wmr.append(wrong_member_rate(labels, labels_true))\n",
    "\n",
    "                fnir, far = np.array(frr) + np.array(wmr), np.array(far)\n",
    "                idxE = np.nanargmin(np.absolute((far - fnir)))\n",
    "                eer  = max(far[idxE],fnir[idxE])*100\n",
    "\n",
    "                eers.append(eer)\n",
    "\n",
    "            mean_eers.append(np.mean(eers))\n",
    "\n",
    "        grid_search.append(mean_eers)\n",
    "        \n",
    "        print(a, threshold, grid_search[-1], sep='  |  ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f94d7",
   "metadata": {
    "cellId": "hvnt4q2pi6pofvdda8n2w"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for a in np.linspace(0.75, 0.95, 3):\n",
    "    for threshold in np.linspace(0.4, 0.8, 5):\n",
    "        print('a =', a, 'threshold =', threshold)\n",
    "        print(grid_search[i])\n",
    "        print()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951a9ce",
   "metadata": {
    "cellId": "k2c90uego9gc0g1c1i0po9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "a, threshold = 0.85, 0.7\n",
    "simulations = 1000\n",
    "\n",
    "\n",
    "mean_eers = []\n",
    "for i in range(len(families)):\n",
    "    eers = []\n",
    "    for j in range(simulations):\n",
    "        fam = list(families[i][j])\n",
    "\n",
    "        enrolls, enrolls_path = generate_enrolls(fam)\n",
    "        train_utts, _, used_in_train = generate_dataset(fam, enrolls_path)\n",
    "        utts, labels_true, _ = generate_dataset(fam, enrolls_path + used_in_train)\n",
    "\n",
    "        #training enrolls\n",
    "        for utterance in train_utts:\n",
    "            for enrolled_id, enrolled_ut in enrolls.items():\n",
    "\n",
    "                if cosine_score(utterance, enrolled_ut) > threshold:\n",
    "                    enrolls[enrolled_id] = F.normalize(a * enrolled_ut + (1 - a) * utterance, p=2, dim=1)\n",
    "\n",
    "        #metrics on test dataset\n",
    "\n",
    "        scores, labels = [], []\n",
    "\n",
    "        for utterance in utts:\n",
    "            scores_with_enrolled = []\n",
    "\n",
    "            for enrolled_id, enrolled_ut in enrolls.items():\n",
    "                scores_with_enrolled.append(cosine_score(utterance, enrolled_ut))\n",
    "\n",
    "            scores.append(max(scores_with_enrolled))\n",
    "            labels.append(list(enrolls.keys())[np.argmax(scores_with_enrolled)])\n",
    "\n",
    "\n",
    "        far, frr, wmr = [], [], []\n",
    "\n",
    "        thresholds = sorted(scores)\n",
    "        labels, scores = np.array(labels), np.array(scores)\n",
    "\n",
    "        for t in thresholds:\n",
    "            labels[np.where(scores <= t)[0]] = 'g'\n",
    "            far.append(false_acceptance_rate(labels, labels_true))\n",
    "            frr.append(false_rejection_rate(labels, labels_true))\n",
    "            wmr.append(wrong_member_rate(labels, labels_true))\n",
    "\n",
    "        fnir, far = np.array(frr) + np.array(wmr), np.array(far)\n",
    "        idxE = np.nanargmin(np.absolute((far - fnir)))\n",
    "        eer  = max(far[idxE],fnir[idxE])*100\n",
    "\n",
    "        eers.append(eer)\n",
    "\n",
    "    mean_eers.append(np.mean(eers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8cdb57",
   "metadata": {
    "cellId": "nuara3kwnrlbzkpscl5cal"
   },
   "outputs": [],
   "source": [
    "#по 1000 семей\n",
    "for i in range(len(mean_eers)):\n",
    "    print('nSpeakers =', i + 2, 'EER =', mean_eers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c254af4",
   "metadata": {
    "cellId": "tnuox1xjdtqevdfs3o3y8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef6b00",
   "metadata": {
    "cellId": "mnjv956wums41zkfzxu6o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434aef74",
   "metadata": {
    "cellId": "6g1umzjbhkb8yjvusjqu8e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b044b4a",
   "metadata": {
    "cellId": "ejnyx9tbcit4mpnec5nlhb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7725dc6",
   "metadata": {
    "cellId": "wpivtw3aksfdlscuqn5jfw"
   },
   "outputs": [],
   "source": [
    "train = open('./data/train_list.txt', 'r')\n",
    "count_utterances = 0\n",
    "\n",
    "for i in train:\n",
    "    count_utterances += 1\n",
    "train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283967e8",
   "metadata": {
    "cellId": "eyy6gm7iiyk3douo6xepre"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train = open('./data/train_list.txt', 'r')\n",
    "test = open('./data/test_list.txt', 'a')\n",
    "\n",
    "i = 0\n",
    "buffer = ''\n",
    "\n",
    "for line in train:\n",
    "    if i % 2:\n",
    "        id_1, path_1 = buffer.split()\n",
    "        id_2, path_2 = line.split()\n",
    "        test.write(str(int(id_1 == id_2)) + ' ' + path_1 + ' ' + path_2 + '\\n')\n",
    "    else:\n",
    "        buffer = line\n",
    "    i += 1\n",
    "    \n",
    "train.close()\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0eb4fb",
   "metadata": {
    "cellId": "jzl0ae8d9clvm9omspng3q"
   },
   "outputs": [],
   "source": [
    "#generates train_list [id, path] to each utterance \n",
    "\n",
    "path = './data/voxceleb1'\n",
    "f = open('./data/train_list.txt', 'a')\n",
    "\n",
    "for id in os.listdir(path):\n",
    "    for youtube in os.listdir(path + '/' + id):\n",
    "        for video_id in os.listdir(path + '/' + id + '/' + youtube):\n",
    "            f.write(id + ' ' + id + '/' + youtube + '/' + video_id + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92875ec",
   "metadata": {
    "cellId": "p77gv0o2rl7ud0e92v95a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notebookId": "e605672c-38c8-4cde-8df8-81c42669be27",
  "notebookPath": "voxceleb_trainer/Work-Copy2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
