{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "081e4bfb",
   "metadata": {
    "cellId": "eo8juvynrpu1g90njw3coe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/work/resources/voxceleb_trainer'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dba757b2",
   "metadata": {
    "cellId": "egufw04b7reaxk00lg7nh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'voxceleb_trainer'\n",
      "/home/jupyter/work/resources/voxceleb_trainer\n"
     ]
    }
   ],
   "source": [
    "%cd voxceleb_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "94c9a747",
   "metadata": {
    "cellId": "gcsjo7hbtzsz7lu1x5iqah"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.9.1+cu111)\n",
      "Requirement already satisfied: torchaudio>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (0.9.1)\n",
      "Requirement already satisfied: numpy in /kernel/fallback/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.19.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (0.22.1)\n",
      "Requirement already satisfied: tqdm in /home/jupyter/.local/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (4.63.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (5.3.1)\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (0.10.3.post1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /kernel/lib/python3.8/site-packages (from soundfile->-r requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /kernel/lib/python3.8/site-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 8)) (2.21)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f87482ef",
   "metadata": {
    "cellId": "74st758ph7fcyzlwwx78ke"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/ 100%[===================>]  10.00G  18.7MB/s    in 10m 31s \n",
      "\n",
      "2022-04-02 18:16:33 (16.2 MB/s) - ‘/home/jupyter/work/resources/voxceleb_trainer/data/vox1_dev_wav_partaa’ saved [10737418240/10737418240]\n",
      "\n",
      "Checksum successful vox1_dev_wav_partaa.\n",
      "--2022-04-02 18:19:37--  https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partab\n",
      "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
      "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10737418240 (10G) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/work/resources/voxceleb_trainer/data/vox1_dev_wav_partab’\n",
      "\n",
      "/home/jupyter/work/ 100%[===================>]  10.00G  20.2MB/s    in 9m 55s  \n",
      "\n",
      "2022-04-02 18:29:33 (17.2 MB/s) - ‘/home/jupyter/work/resources/voxceleb_trainer/data/vox1_dev_wav_partab’ saved [10737418240/10737418240]\n",
      "\n",
      "Checksum successful vox1_dev_wav_partab.\n",
      "--2022-04-02 18:32:40--  https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partac\n",
      "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
      "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10737418240 (10G) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/work/resources/voxceleb_trainer/data/vox1_dev_wav_partac’\n",
      "\n",
      "/home/jupyter/work/ 100%[===================>]  10.00G  29.3MB/s    in 10m 5s  \n",
      "\n",
      "2022-04-02 18:42:45 (16.9 MB/s) - ‘/home/jupyter/work/resources/voxceleb_trainer/data/vox1_dev_wav_partac’ saved [10737418240/10737418240]\n",
      "\n",
      "Checksum successful vox1_dev_wav_partac.\n",
      "--2022-04-02 18:45:49--  https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partad\n",
      "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
      "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 384369463 (367M) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/work/resources/voxceleb_trainer/data/vox1_dev_wav_partad’\n",
      "\n",
      "/home/jupyter/work/ 100%[===================>] 366.56M  32.3MB/s    in 33s     \n",
      "\n",
      "2022-04-02 18:46:22 (11.3 MB/s) - ‘/home/jupyter/work/resources/voxceleb_trainer/data/vox1_dev_wav_partad’ saved [384369463/384369463]\n",
      "\n",
      "Checksum successful vox1_dev_wav_partad.\n",
      "--2022-04-02 18:46:26--  https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip\n",
      "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
      "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1072793438 (1023M) [application/zip]\n",
      "Saving to: ‘/home/jupyter/work/resources/voxceleb_trainer/data/vox1_test_wav.zip’\n",
      "\n",
      "/home/jupyter/work/ 100%[===================>]   1023M  14.0MB/s    in 55s     \n",
      "\n",
      "2022-04-02 18:47:21 (18.6 MB/s) - ‘/home/jupyter/work/resources/voxceleb_trainer/data/vox1_test_wav.zip’ saved [1072793438/1072793438]\n",
      "\n",
      "Checksum successful vox1_test_wav.zip.\n"
     ]
    }
   ],
   "source": [
    "!python3 ./dataprep.py --save_path /home/jupyter/work/resources/voxceleb_trainer/data --download --user stakhova7 --password dreamjob7  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "261433d9",
   "metadata": {
    "cellId": "78rp2p891x4xevh8v7z7u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksum successful vox1_dev_wav.zip.\n",
      "Extracting /home/jupyter/work/resources/voxceleb_trainer/data/vox1_dev_wav.zip\n",
      "mv: cannot stat '/home/jupyter/work/resources/voxceleb_trainer/data/dev/aac/*': No such file or directory\n",
      "mv: cannot stat '/home/jupyter/work/resources/voxceleb_trainer/data/aac': No such file or directory\n",
      "Converting files from AAC to WAV\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 ./dataprep.py --save_path /home/jupyter/work/resources/voxceleb_trainer/data --extract\n",
    "!python3 ./dataprep.py --save_path /home/jupyter/work/resources/voxceleb_trainer/data --convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1f1ce51f",
   "metadata": {
    "cellId": "6onzlhczjt313q71jei80n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.8.12 (default, Sep 10 2021, 00:16:05) \n",
      "[GCC 7.5.0]\n",
      "PyTorch Version: 1.9.1+cu111\n",
      "Number of GPUs: 1\n",
      "Save path: /home/jupyter/work/resources/voxceleb_trainer/exps/test\n",
      "Embedding size is 512, encoder ASP.\n",
      "Initialised Softmax Loss\n",
      "Initialised AngleProto\n",
      "Initialised SoftmaxPrototypical Loss\n",
      "Initialised Adam optimizer\n",
      "Initialised step LR scheduler\n",
      "Model baseline_v2_ap.model loaded!\n",
      "Total parameters:  8028492\n",
      "Test list data/test_list.txt\n",
      "Reading 0 of 6: 0.00 Hz, embedding size 512\n",
      "normalized\n",
      "Computing 0 of 3: 0.00 Hznormalized\n",
      "normalized\n",
      "Traceback (most recent call last):\n",
      "  File \"./trainSpeakerNet.py\", line 314, in <module>\n",
      "    main()\n",
      "  File \"./trainSpeakerNet.py\", line 310, in main\n",
      "    main_worker(0, None, args)\n",
      "  File \"./trainSpeakerNet.py\", line 200, in main_worker\n",
      "    result = tuneThresholdfromScore(sc, lab, [1, 0.1]);\n",
      "  File \"/home/jupyter/work/resources/voxceleb_trainer/tuneThreshold.py\", line 25, in tuneThresholdfromScore\n",
      "    idx = numpy.nanargmin(numpy.absolute((tfa - fpr))) # numpy.where(fpr<=tfa)[0][-1]\n",
      "  File \"<__array_function__ internals>\", line 5, in nanargmin\n",
      "  File \"/kernel/lib/python3.8/site-packages/numpy/lib/nanfunctions.py\", line 499, in nanargmin\n",
      "    raise ValueError(\"All-NaN slice encountered\")\n",
      "ValueError: All-NaN slice encountered\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Process exited with code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0aefc28fc6bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python3 ./trainSpeakerNet.py --eval --model ResNetSE34V2 --log_input True --encoder_type ASP --n_mels 64 --trainfunc softmaxproto --save_path /home/jupyter/work/resources/voxceleb_trainer/exps/test --eval_frames 400  --initial_model baseline_v2_ap.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.8/site-packages/ml_kernel/kernel.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_script_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScriptExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProcessHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_script_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_message_handlers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.8/site-packages/ml_kernel/script_executor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, lang, code)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mreturn_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Process exited with code %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreturn_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: Process exited with code 1"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!python3 ./trainSpeakerNet.py --eval --model ResNetSE34V2 --log_input True --encoder_type ASP --n_mels 64 --trainfunc softmaxproto --save_path /home/jupyter/work/resources/voxceleb_trainer/exps/test --eval_frames 400  --initial_model baseline_v2_ap.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c6ba37",
   "metadata": {
    "cellId": "pcpnuo051wnzcp2ksl1m9q"
   },
   "source": [
    "## Генерируем семьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f97dbd6e",
   "metadata": {
    "cellId": "b7lndp82y3swq8crrm932b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "embs = torch.load('embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "f8a675b8",
   "metadata": {
    "cellId": "jxtapjzslkct8zln4jgly8"
   },
   "outputs": [],
   "source": [
    "#делаем словарь вида {'id' : усредненный embedding диктора}\n",
    "\n",
    "utterance_list = open('./data/train_list.txt', 'r')\n",
    "\n",
    "mean_embs = dict()\n",
    "current_id = ''\n",
    "emb_sum, count = 0, 0\n",
    "\n",
    "for line in utterance_list:\n",
    "    id, path = line.split()\n",
    "    \n",
    "    if id == current_id:\n",
    "        emb_sum += embs[path]\n",
    "        count += 1\n",
    "    else:\n",
    "        if current_id:\n",
    "            mean_embs[current_id] = 1 / count * emb_sum\n",
    "\n",
    "        emb_sum = embs[path]\n",
    "        count = 1\n",
    "        current_id = id\n",
    "        \n",
    "    mean_embs[current_id] = 1 / count * emb_sum\n",
    "        \n",
    "utterance_list.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b94526c5",
   "metadata": {
    "cellId": "vj7rqpggnzfu3lgyetf38"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#score каждого диктора с каждым\n",
    "import numpy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "scores = dict()\n",
    "ids = list(mean_embs.keys())\n",
    "\n",
    "for i in range(len(ids)):\n",
    "    for j in range(i + 1, len(ids)):\n",
    "        \n",
    "        ref_feat = mean_embs[ids[i]].cuda()\n",
    "        com_feat = mean_embs[ids[j]].cuda()\n",
    "\n",
    "        dist = F.cosine_similarity(ref_feat.unsqueeze(-1), com_feat.unsqueeze(-1).transpose(0,2)).detach().cpu().numpy();\n",
    "        score = numpy.mean(dist);\n",
    "        scores[(ids[i], ids[j])] = score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "5a8c09db",
   "metadata": {
    "cellId": "6j1zizqabvnux67he3z9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.19410758 0.8223336\n"
     ]
    }
   ],
   "source": [
    "similarity_scores = list(scores.values())\n",
    "min_score = min(similarity_scores)\n",
    "max_score = max(similarity_scores)\n",
    "print(min_score, max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "f85a6e6a",
   "metadata": {
    "cellId": "846jnjw3ced28txyss3mft"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#делаем матрицу scores\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "ids = list(mean_embs.keys())\n",
    "\n",
    "n = len(ids)\n",
    "scores = np.zeros((n, n))\n",
    "id2ind = {ids[i] : i for i in range(n)}\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        \n",
    "        ref_feat = mean_embs[ids[i]].cuda()\n",
    "        com_feat = mean_embs[ids[j]].cuda()\n",
    "\n",
    "        dist = F.cosine_similarity(ref_feat.unsqueeze(-1), com_feat.unsqueeze(-1).transpose(0,2)).detach().cpu().numpy();\n",
    "        score = np.mean(dist);\n",
    "        scores[id2ind[ids[i]], id2ind[ids[j]]] = score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "50a29835",
   "metadata": {
    "cellId": "6ynlmqshbrrzdgwrxwqmqf"
   },
   "outputs": [],
   "source": [
    "scores = scores + scores.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a8439445",
   "metadata": {
    "cellId": "lomf9kc9paig6oks3mp5bf"
   },
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "ind2id = {i : ids[i] for i in range(n)}\n",
    "\n",
    "components_mat = scores > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "2dcc1869",
   "metadata": {
    "cellId": "umwonuc5x1l5dypmpqmte"
   },
   "outputs": [],
   "source": [
    "def generate_family(member_id, member_count, components_mat, id2ind, ind2id, relatives_set=set(ids)):\n",
    "    \n",
    "    family = {member_id}\n",
    "    \n",
    "    while len(family) < len(relatives_set):\n",
    "        related = np.nonzero(components_mat[id2ind[member_id], :])[0]\n",
    "        related_ids = [ind2id[i] for i in related]\n",
    "\n",
    "        relatives_set &= set(related_ids)\n",
    "        \n",
    "        if len(family) == member_count:\n",
    "            return family\n",
    "        if not relatives_set:\n",
    "            return family\n",
    "        \n",
    "        add_member_id = np.random.choice(list(relatives_set))\n",
    "        \n",
    "        #print('current member ', member_id, ' related to ', relatives_set, ' chosen connection to ', add_member_id)\n",
    "        \n",
    "        family.add(add_member_id)\n",
    "        member_id = add_member_id\n",
    "        \n",
    "    return family\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ac48ed44",
   "metadata": {
    "cellId": "zugg6dgsdwi4x71i8er8ta"
   },
   "outputs": [],
   "source": [
    "families = []\n",
    "\n",
    "for member_count in range(2, 8):\n",
    "    fixed_size_families = []\n",
    "    for id_ in ids:\n",
    "        family = generate_family(id_, member_count, components_mat.astype(int), id2ind, ind2id, relatives_set=set(ids))\n",
    "        if len(family) == member_count:\n",
    "            fixed_size_families.append(family)\n",
    "    families.append(fixed_size_families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "02e0bb59",
   "metadata": {
    "cellId": "uilo79233hfmlpn1z5utk"
   },
   "outputs": [],
   "source": [
    "#generate_family('id10104', components_mat.astype(int), id2ind, ind2id, relatives_set=set(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "ed4b44f8",
   "metadata": {
    "cellId": "mtblgjqogrsqorecobyuj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nSpeakers 2 1211\n",
      "nSpeakers 3 1211\n",
      "nSpeakers 4 1210\n",
      "nSpeakers 5 1206\n",
      "nSpeakers 6 1203\n",
      "nSpeakers 7 1189\n"
     ]
    }
   ],
   "source": [
    "for i, fixed_families in enumerate(families):\n",
    "    print('nSpeakers', i + 2, len(np.unique(fixed_families)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf23cc81",
   "metadata": {
    "cellId": "p4h5vx3ts7sf8uch7cgpl5"
   },
   "source": [
    "## Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "1a9b7631",
   "metadata": {
    "cellId": "095uusrw6bslc03lkp1zx05"
   },
   "outputs": [],
   "source": [
    "def tuneThresholdfromScore(scores, labels, target_fa, target_fr = None):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    tunedThreshold = [];\n",
    "    if target_fr:\n",
    "        for tfr in target_fr:\n",
    "            idx = numpy.nanargmin(numpy.absolute((tfr - fnr)))\n",
    "            tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]]);\n",
    "    \n",
    "    for tfa in target_fa:\n",
    "        idx = numpy.nanargmin(numpy.absolute((tfa - fpr))) # numpy.where(fpr<=tfa)[0][-1]\n",
    "        tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]]);\n",
    "    \n",
    "    idxE = numpy.nanargmin(numpy.absolute((fnr - fpr)))\n",
    "    eer  = max(fpr[idxE],fnr[idxE])*100\n",
    "    \n",
    "    return (tunedThreshold, eer, fpr, fnr);\n",
    "\n",
    "# Creates a list of false-negative rates, a list of false-positive rates\n",
    "# and a list of decision thresholds that give those error-rates.\n",
    "\n",
    "\n",
    "def ComputeErrorRates(scores, labels):\n",
    "\n",
    "      # Sort the scores from smallest to largest, and also get the corresponding\n",
    "      # indexes of the sorted scores.  We will treat the sorted scores as the\n",
    "      # thresholds at which the the error-rates are evaluated.\n",
    "      sorted_indexes, thresholds = zip(*sorted(\n",
    "          [(index, threshold) for index, threshold in enumerate(scores)],\n",
    "          key=itemgetter(1)))\n",
    "      sorted_labels = []\n",
    "      labels = [labels[i] for i in sorted_indexes]\n",
    "      fnrs = []\n",
    "      fprs = []\n",
    "\n",
    "      # At the end of this loop, fnrs[i] is the number of errors made by\n",
    "      # incorrectly rejecting scores less than thresholds[i]. And, fprs[i]\n",
    "      # is the total number of times that we have correctly accepted scores\n",
    "      # greater than thresholds[i].\n",
    "      for i in range(0, len(labels)):\n",
    "          if i == 0:\n",
    "              fnrs.append(labels[i])\n",
    "              fprs.append(1 - labels[i])\n",
    "          else:\n",
    "              fnrs.append(fnrs[i-1] + labels[i])\n",
    "              fprs.append(fprs[i-1] + 1 - labels[i])\n",
    "      fnrs_norm = sum(labels)\n",
    "      fprs_norm = len(labels) - fnrs_norm\n",
    "\n",
    "      # Now divide by the total number of false negative errors to\n",
    "      # obtain the false positive rates across all thresholds\n",
    "      fnrs = [x / float(fnrs_norm) for x in fnrs]\n",
    "\n",
    "      # Divide by the total number of corret positives to get the\n",
    "      # true positive rate.  Subtract these quantities from 1 to\n",
    "      # get the false positive rates.\n",
    "      fprs = [1 - x / float(fprs_norm) for x in fprs]\n",
    "      return fnrs, fprs, thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "ca65ac1d",
   "metadata": {
    "cellId": "flhkwr1c3qgq1xl0tkzyt"
   },
   "outputs": [],
   "source": [
    "def false_acceptance_rate(labels, labels_true):\n",
    "    labels, labels_true = np.array(labels), np.array(labels_true)\n",
    "    return ((labels != 'g') & (labels_true == 'g')).sum() / len(labels) \n",
    "\n",
    "\n",
    "def false_rejection_rate(labels, labels_true):\n",
    "    labels, labels_true = np.array(labels), np.array(labels_true)\n",
    "    return ((labels == 'g') & (labels_true != 'g')).sum() / len(labels) \n",
    "\n",
    "\n",
    "def wrong_member_rate(labels, labels_true):\n",
    "    labels, labels_true = np.array(labels), np.array(labels_true)\n",
    "    return ((labels != 'g') & (labels_true != 'g') & (labels != labels_true)).sum() / (labels_true != 'g').sum() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "065b6bea",
   "metadata": {
    "cellId": "xtfvrwwuswf22waur1vo04"
   },
   "outputs": [],
   "source": [
    "#сделаем словарь вида {id : [list of path to records]}\n",
    "\n",
    "f = open('./data/train_list.txt', 'r')\n",
    "id2records = {ids[i] : [] for i in range(len(ids))}\n",
    "\n",
    "for line in f:\n",
    "    id_, path = line.split()\n",
    "    id2records[id_].append(path)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "1a8c9f28",
   "metadata": {
    "cellId": "429ilcxv68tzb27n5vd3ak"
   },
   "outputs": [],
   "source": [
    "#сделаем пробную генерацию выборки и пропустим ее через tuneThresholdfromScore\n",
    "#необходимо сделать так, чтобы эмбеддинги из enrolls не пересекались с эмбеддингами из dataset\n",
    "\n",
    "#member_id - person id \n",
    "#count - len(list of utterances from person_id)\n",
    "\n",
    "def sample_utterance(member_id, count, except_paths=[]):\n",
    "    path_records = list(np.random.choice(list(set(id2records[member_id]) - set(except_paths)), count))\n",
    "    sample = [embs[path] for path in path_records]\n",
    "    return sample, path_records\n",
    "\n",
    "def generate_enrolls(family_ids, level=4):\n",
    "    enrolls = dict.fromkeys(family_ids)\n",
    "    for member_id in family_ids:\n",
    "        utterances, path_records = sample_utterance(member_id, level)\n",
    "        enrolls[member_id] = sum(utterances) / level\n",
    "    return enrolls, path_records\n",
    "    \n",
    "from sklearn.utils import shuffle  \n",
    "def generate_dataset(family_ids, enrolled_paths, ut_per_member=10, ut_per_guest=4, guests_count=4):\n",
    "    utterances, labels, used_records = [], [], []\n",
    "    for member_id in family_ids:\n",
    "        sample, paths = sample_utterance(member_id, ut_per_member, enrolled_paths)\n",
    "        \n",
    "        utterances += sample\n",
    "        used_records += paths\n",
    "        labels += [member_id] * ut_per_member\n",
    "        \n",
    "    guest_ids = np.random.choice(list(set(ids) - set(family_ids)), guests_count)\n",
    "    for guest_id in guest_ids:\n",
    "        sample, paths = sample_utterance(guest_id, ut_per_guest, enrolled_paths)\n",
    "        utterances += sample\n",
    "        used_records += paths\n",
    "        labels += ['g'] * ut_per_guest\n",
    "    \n",
    "    utterances, labels = shuffle(utterances, labels)\n",
    "    return utterances, labels, used_records\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "1d63d908",
   "metadata": {
    "cellId": "zsxuztyqokigviwvs4pltk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#берем по 100 примеров каждой семьи\n",
    "import torch.nn.functional as F\n",
    "\n",
    "mean_eers = []\n",
    "\n",
    "for i in range(len(families)):\n",
    "    eers = []\n",
    "    for j in range(1000):\n",
    "        fam = list(families[i][j])\n",
    "        enrolls, enrolls_path = generate_enrolls(fam)\n",
    "        utts, labels_true, _ = generate_dataset(fam, enrolls_path)\n",
    "\n",
    "        scores = []\n",
    "        labels = []\n",
    "\n",
    "        for utterance in utts:\n",
    "            scores_with_enrolled = []\n",
    "            for enrolled_id, enrolled_ut in enrolls.items():\n",
    "                ref_feat = utterance.cuda()\n",
    "                com_feat = enrolled_ut.cuda()\n",
    "                dist = F.cosine_similarity(ref_feat.unsqueeze(-1), com_feat.unsqueeze(-1).transpose(0,2)).detach().cpu().numpy();\n",
    "                score = numpy.mean(dist);\n",
    "                scores_with_enrolled.append(score)\n",
    "\n",
    "            scores.append(max(scores_with_enrolled))\n",
    "            labels.append(list(enrolls.keys())[np.argmax(scores_with_enrolled)])\n",
    "\n",
    "\n",
    "        far, frr, wmr = [], [], []\n",
    "\n",
    "        thresholds = sorted(scores)\n",
    "        labels, scores = np.array(labels), np.array(scores)\n",
    "\n",
    "        for t in thresholds:\n",
    "            labels[np.where(scores <= t)[0]] = 'g'\n",
    "            far.append(false_acceptance_rate(labels, labels_true))\n",
    "            frr.append(false_rejection_rate(labels, labels_true))\n",
    "            wmr.append(wrong_member_rate(labels, labels_true))\n",
    "\n",
    "        fnir, far = np.array(frr) + np.array(wmr), np.array(far)\n",
    "        idxE = np.nanargmin(np.absolute((far - fnir)))\n",
    "        eer  = max(far[idxE],fnir[idxE])*100\n",
    "        eers.append(eer)\n",
    "    mean_eers.append(np.mean(eers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "417d8b0b",
   "metadata": {
    "cellId": "sjb1u93o5m5mqmr10q0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nSpeakers = 2 EER = 0.33333333333333326\n",
      "nSpeakers = 3 EER = 0.48115942028985503\n",
      "nSpeakers = 4 EER = 0.23928571428571427\n",
      "nSpeakers = 5 EER = 0.40848484848484845\n",
      "nSpeakers = 6 EER = 0.8350877192982454\n",
      "nSpeakers = 7 EER = 0.6714285714285714\n"
     ]
    }
   ],
   "source": [
    "#по 100 семей\n",
    "for i in range(len(mean_eers)):\n",
    "    print('nSpeakers =', i + 2, 'EER =', mean_eers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "921e2c58",
   "metadata": {
    "cellId": "ao3vtwsmu0siohbdrs7etm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nSpeakers = 2 EER = 0.2716666666666666\n",
      "nSpeakers = 3 EER = 0.3307246376811594\n",
      "nSpeakers = 4 EER = 0.41535714285714276\n",
      "nSpeakers = 5 EER = 0.5258787878787878\n",
      "nSpeakers = 6 EER = 0.5651754385964912\n",
      "nSpeakers = 7 EER = 0.5977740863787376\n"
     ]
    }
   ],
   "source": [
    "#по 1000 семей\n",
    "for i in range(len(mean_eers)):\n",
    "    print('nSpeakers =', i + 2, 'EER =', mean_eers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "95c04f59",
   "metadata": {
    "cellId": "5wbmwyxmd6myymt8t0kzrp"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_score(emb_1, emb_2):\n",
    "    ref_feat = emb_1.cuda()\n",
    "    com_feat = emb_2.cuda()\n",
    "    dist = F.cosine_similarity(ref_feat.unsqueeze(-1), com_feat.unsqueeze(-1).transpose(0,2)).detach().cpu().numpy();\n",
    "    score = numpy.mean(dist);\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66affc75",
   "metadata": {
    "cellId": "ifkmtmbg67bv7cb6b55sz9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75  |  0.4  |  [2.994444444444444, 13.463768115942027, 14.492857142857142, 21.97151515151515, 27.88157894736842, 32.275415282392025]\n",
      "0.75  |  0.5  |  [1.4722222222222223, 1.7376811594202897, 1.775, 4.035757575757575, 4.95701754385965, 3.785382059800665]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "simulations = 100\n",
    "grid_search = []\n",
    "\n",
    "for a in np.linspace(0.75, 0.95, 3):\n",
    "    for threshold in np.linspace(0.4, 0.8, 5):\n",
    "\n",
    "        mean_eers = []\n",
    "        for i in range(len(families)):\n",
    "            eers = []\n",
    "            for j in range(simulations):\n",
    "                fam = list(families[i][j])\n",
    "\n",
    "                enrolls, enrolls_path = generate_enrolls(fam)\n",
    "                train_utts, _, used_in_train = generate_dataset(fam, enrolls_path)\n",
    "                utts, labels_true, _ = generate_dataset(fam, enrolls_path + used_in_train)\n",
    "\n",
    "                #training enrolls\n",
    "                for utterance in train_utts:\n",
    "                    for enrolled_id, enrolled_ut in enrolls.items():\n",
    "\n",
    "                        if cosine_score(utterance, enrolled_ut) > threshold:\n",
    "                            enrolls[enrolled_id] = F.normalize(a * enrolled_ut + (1 - a) * utterance, p=2, dim=1)\n",
    "\n",
    "                #metrics on test dataset\n",
    "\n",
    "                scores, labels = [], []\n",
    "\n",
    "                for utterance in utts:\n",
    "                    scores_with_enrolled = []\n",
    "\n",
    "                    for enrolled_id, enrolled_ut in enrolls.items():\n",
    "                        scores_with_enrolled.append(cosine_score(utterance, enrolled_ut))\n",
    "\n",
    "                    scores.append(max(scores_with_enrolled))\n",
    "                    labels.append(list(enrolls.keys())[np.argmax(scores_with_enrolled)])\n",
    "\n",
    "\n",
    "                far, frr, wmr = [], [], []\n",
    "\n",
    "                thresholds = sorted(scores)\n",
    "                labels, scores = np.array(labels), np.array(scores)\n",
    "\n",
    "                for t in thresholds:\n",
    "                    labels[np.where(scores <= t)[0]] = 'g'\n",
    "                    far.append(false_acceptance_rate(labels, labels_true))\n",
    "                    frr.append(false_rejection_rate(labels, labels_true))\n",
    "                    wmr.append(wrong_member_rate(labels, labels_true))\n",
    "\n",
    "                fnir, far = np.array(frr) + np.array(wmr), np.array(far)\n",
    "                idxE = np.nanargmin(np.absolute((far - fnir)))\n",
    "                eer  = max(far[idxE],fnir[idxE])*100\n",
    "\n",
    "                eers.append(eer)\n",
    "\n",
    "            mean_eers.append(np.mean(eers))\n",
    "\n",
    "        grid_search.append(mean_eers)\n",
    "        \n",
    "        print(a, threshold, grid_search[-1], sep='  |  ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "148f94d7",
   "metadata": {
    "cellId": "hvnt4q2pi6pofvdda8n2w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 0.75 threshold = 0.4\n",
      "[1.4444444444444446, 6.8478260869565215, 6.946428571428572, 13.093333333333332, 19.71228070175438, 23.912292358803985]\n",
      "\n",
      "a = 0.75 threshold = 0.5\n",
      "[0.8055555555555556, 1.0869565217391304, 1.4964285714285714, 3.213333333333334, 5.185964912280701, 5.819933554817275]\n",
      "\n",
      "a = 0.75 threshold = 0.6000000000000001\n",
      "[0.47222222222222215, 0.5231884057971015, 0.4107142857142857, 1.1866666666666668, 1.1692982456140348, 0.7544850498338872]\n",
      "\n",
      "a = 0.75 threshold = 0.7000000000000001\n",
      "[0.33333333333333326, 0.34782608695652173, 0.3928571428571428, 0.509090909090909, 0.6263157894736842, 0.8259136212624584]\n",
      "\n",
      "a = 0.75 threshold = 0.8\n",
      "[0.8611111111111112, 0.43623188405797103, 0.3928571428571428, 0.5048484848484849, 0.6763157894736841, 0.764451827242525]\n",
      "\n",
      "a = 0.85 threshold = 0.4\n",
      "[1.5888888888888888, 6.230434782608696, 8.239285714285714, 14.48969696969697, 16.923684210526318, 25.39435215946844]\n",
      "\n",
      "a = 0.85 threshold = 0.5\n",
      "[0.8388888888888889, 1.2478260869565219, 1.675, 2.7624242424242427, 4.3464912280701755, 5.836544850498339]\n",
      "\n",
      "a = 0.85 threshold = 0.6000000000000001\n",
      "[0.5833333333333334, 0.3956521739130434, 0.2964285714285714, 1.064848484848485, 1.479824561403509, 1.3043189368770762]\n",
      "\n",
      "a = 0.85 threshold = 0.7000000000000001\n",
      "[0.19444444444444448, 0.30434782608695654, 0.3714285714285714, 0.42363636363636364, 0.7157894736842105, 0.6970099667774086]\n",
      "\n",
      "a = 0.85 threshold = 0.8\n",
      "[0.34444444444444444, 0.32753623188405795, 0.5964285714285713, 0.41272727272727283, 0.669298245614035, 0.7152823920265782]\n",
      "\n",
      "a = 0.95 threshold = 0.4\n",
      "[1.5833333333333335, 2.8840579710144927, 3.2142857142857144, 6.573939393939392, 7.930701754385965, 12.722591362126245]\n",
      "\n",
      "a = 0.95 threshold = 0.5\n",
      "[0.05555555555555555, 0.8724637681159418, 0.8392857142857143, 2.026060606060606, 2.77280701754386, 2.935548172757475]\n",
      "\n",
      "a = 0.95 threshold = 0.6000000000000001\n",
      "[0.25, 0.608695652173913, 0.46071428571428563, 1.2696969696969698, 1.2850877192982455, 0.8431893687707642]\n",
      "\n",
      "a = 0.95 threshold = 0.7000000000000001\n",
      "[0.41666666666666663, 0.22898550724637678, 0.3607142857142857, 0.4733333333333334, 0.5508771929824561, 0.7787375415282394]\n",
      "\n",
      "a = 0.95 threshold = 0.8\n",
      "[0.4444444444444444, 0.2826086956521739, 0.28214285714285714, 0.3218181818181819, 0.6359649122807017, 0.7787375415282392]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for a in np.linspace(0.75, 0.95, 3):\n",
    "    for threshold in np.linspace(0.4, 0.8, 5):\n",
    "        print('a =', a, 'threshold =', threshold)\n",
    "        print(grid_search[i])\n",
    "        print()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "c951a9ce",
   "metadata": {
    "cellId": "k2c90uego9gc0g1c1i0po9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "a, threshold = 0.85, 0.7\n",
    "simulations = 1000\n",
    "\n",
    "\n",
    "mean_eers = []\n",
    "for i in range(len(families)):\n",
    "    eers = []\n",
    "    for j in range(simulations):\n",
    "        fam = list(families[i][j])\n",
    "\n",
    "        enrolls, enrolls_path = generate_enrolls(fam)\n",
    "        train_utts, _, used_in_train = generate_dataset(fam, enrolls_path)\n",
    "        utts, labels_true, _ = generate_dataset(fam, enrolls_path + used_in_train)\n",
    "\n",
    "        #training enrolls\n",
    "        for utterance in train_utts:\n",
    "            for enrolled_id, enrolled_ut in enrolls.items():\n",
    "\n",
    "                if cosine_score(utterance, enrolled_ut) > threshold:\n",
    "                    enrolls[enrolled_id] = F.normalize(a * enrolled_ut + (1 - a) * utterance, p=2, dim=1)\n",
    "\n",
    "        #metrics on test dataset\n",
    "\n",
    "        scores, labels = [], []\n",
    "\n",
    "        for utterance in utts:\n",
    "            scores_with_enrolled = []\n",
    "\n",
    "            for enrolled_id, enrolled_ut in enrolls.items():\n",
    "                scores_with_enrolled.append(cosine_score(utterance, enrolled_ut))\n",
    "\n",
    "            scores.append(max(scores_with_enrolled))\n",
    "            labels.append(list(enrolls.keys())[np.argmax(scores_with_enrolled)])\n",
    "\n",
    "\n",
    "        far, frr, wmr = [], [], []\n",
    "\n",
    "        thresholds = sorted(scores)\n",
    "        labels, scores = np.array(labels), np.array(scores)\n",
    "\n",
    "        for t in thresholds:\n",
    "            labels[np.where(scores <= t)[0]] = 'g'\n",
    "            far.append(false_acceptance_rate(labels, labels_true))\n",
    "            frr.append(false_rejection_rate(labels, labels_true))\n",
    "            wmr.append(wrong_member_rate(labels, labels_true))\n",
    "\n",
    "        fnir, far = np.array(frr) + np.array(wmr), np.array(far)\n",
    "        idxE = np.nanargmin(np.absolute((far - fnir)))\n",
    "        eer  = max(far[idxE],fnir[idxE])*100\n",
    "\n",
    "        eers.append(eer)\n",
    "\n",
    "    mean_eers.append(np.mean(eers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "dd8cdb57",
   "metadata": {
    "cellId": "nuara3kwnrlbzkpscl5cal"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nSpeakers = 2 EER = 0.41777777777777775\n",
      "nSpeakers = 3 EER = 0.5278260869565218\n",
      "nSpeakers = 4 EER = 0.7357142857142857\n",
      "nSpeakers = 5 EER = 0.796\n",
      "nSpeakers = 6 EER = 0.9146491228070174\n",
      "nSpeakers = 7 EER = 1.0613953488372092\n"
     ]
    }
   ],
   "source": [
    "#по 1000 семей\n",
    "for i in range(len(mean_eers)):\n",
    "    print('nSpeakers =', i + 2, 'EER =', mean_eers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c254af4",
   "metadata": {
    "cellId": "tnuox1xjdtqevdfs3o3y8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef6b00",
   "metadata": {
    "cellId": "mnjv956wums41zkfzxu6o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434aef74",
   "metadata": {
    "cellId": "6g1umzjbhkb8yjvusjqu8e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b044b4a",
   "metadata": {
    "cellId": "ejnyx9tbcit4mpnec5nlhb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c7725dc6",
   "metadata": {
    "cellId": "wpivtw3aksfdlscuqn5jfw"
   },
   "outputs": [],
   "source": [
    "train = open('./data/train_list.txt', 'r')\n",
    "count_utterances = 0\n",
    "\n",
    "for i in train:\n",
    "    count_utterances += 1\n",
    "train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "283967e8",
   "metadata": {
    "cellId": "eyy6gm7iiyk3douo6xepre"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train = open('./data/train_list.txt', 'r')\n",
    "test = open('./data/test_list.txt', 'a')\n",
    "\n",
    "i = 0\n",
    "buffer = ''\n",
    "\n",
    "for line in train:\n",
    "    if i % 2:\n",
    "        id_1, path_1 = buffer.split()\n",
    "        id_2, path_2 = line.split()\n",
    "        test.write(str(int(id_1 == id_2)) + ' ' + path_1 + ' ' + path_2 + '\\n')\n",
    "    else:\n",
    "        buffer = line\n",
    "    i += 1\n",
    "    \n",
    "train.close()\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f0eb4fb",
   "metadata": {
    "cellId": "jzl0ae8d9clvm9omspng3q"
   },
   "outputs": [],
   "source": [
    "#generates train_list [id, path] to each utterance \n",
    "\n",
    "path = './data/voxceleb1'\n",
    "f = open('./data/train_list.txt', 'a')\n",
    "\n",
    "for id in os.listdir(path):\n",
    "    for youtube in os.listdir(path + '/' + id):\n",
    "        for video_id in os.listdir(path + '/' + id + '/' + youtube):\n",
    "            f.write(id + ' ' + id + '/' + youtube + '/' + video_id + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92875ec",
   "metadata": {
    "cellId": "p77gv0o2rl7ud0e92v95a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notebookId": "e605672c-38c8-4cde-8df8-81c42669be27",
  "notebookPath": "voxceleb_trainer/Work-Copy2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
